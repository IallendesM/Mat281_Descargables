{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrRjlKIDDprH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fralfaro/MAT281_2023/blob/main/homeworks/h1/hw_01.ipynb\n",
        "\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "# Tarea N¬∞02\n",
        "\n",
        "\n",
        "## Instrucciones\n",
        "\n",
        "1.- Completa tus datos personales (nombre y rol USM) en siguiente celda.\n",
        "\n",
        "\n",
        "* __Nombre__: Ignacio Allendes\n",
        "\n",
        "* __Rol__: 202004531-3\n",
        "\n",
        "2.- Debes _subir_ este archivo con tus cambios a tu repositorio personal del curso, incluyendo datos, im√°genes, scripts, etc.\n",
        "\n",
        "3.- Se evaluar√°:\n",
        "   - Soluciones\n",
        "   - C√≥digo\n",
        "   - Al presionar  `Kernel -> Restart Kernel and Run All Cells` deben ejecutarse todas las celdas sin error.\n",
        "   \n",
        "4.- Esta Tarea debe ser entregada en **Dos Jupyter Notebooks Distinto**.\n",
        "   * **Ejemplo**: `hw_02_part_01.ipynb`, `hw_02_part_02.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UthZEe_eDprO"
      },
      "source": [
        "## II.- Titanic - Machine Learning from Disaster\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1dsHUmSqyCYvez1LRlieo_y06haGL6t9r\" width = \"300\" align=\"center\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twu62YbEDprO"
      },
      "source": [
        "### üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place.\n",
        "\n",
        "This is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n",
        "\n",
        "The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n",
        "\n",
        "Read on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the [\"Join Competition button](https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic) to create an account and gain access to the [competition data](https://www.kaggle.com/c/titanic/data). Then check out [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through step by step how to make your first submission!\n",
        "\n",
        "  <a href=\"https://www.youtube.com/watch?v=8yZMXCaFshs&ab_channel=Kaggle\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1CZdve-0JeOxk7n09Bz0M5SC1Ae9SsRoe\" width = \"600\">\n",
        "  </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG39KeGdDprP"
      },
      "source": [
        "###  The Challenge\n",
        "\n",
        "\n",
        "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
        "\n",
        "On April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
        "\n",
        "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
        "\n",
        "In this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n",
        "\n",
        "> #### Recommended Tutorial\n",
        ">\n",
        "> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n",
        "\n",
        "###  Overview of How Kaggle‚Äôs Competitions Work\n",
        "\n",
        "1.  **Join the Competition**  \n",
        "    Read about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n",
        "2.  **Get to Work**  \n",
        "    Download the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n",
        "3.  **Make a Submission**  \n",
        "    Upload your prediction as a submission on Kaggle and receive an accuracy score.\n",
        "4.  **Check the Leaderboard**  \n",
        "    See how your model ranks against other Kagglers on our leaderboard.\n",
        "5.  **Improve Your Score**  \n",
        "    Check out the [discussion forum](https://www.kaggle.com/c/titanic/discussion) to find lots of tutorials and insights from other competitors.\n",
        "\n",
        "> #### Kaggle Lingo Video\n",
        ">\n",
        "> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n",
        "\n",
        "###  What Data Will I Use in This Competition?\n",
        "\n",
        "In this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n",
        "\n",
        "`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n",
        "\n",
        "The `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n",
        "\n",
        "Using the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n",
        "\n",
        "Check out the [‚ÄúData‚Äù tab](https://www.kaggle.com/c/titanic/data) to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n",
        "\n",
        "###  How to Submit your Prediction to Kaggle\n",
        "\n",
        "Once you‚Äôre ready to make a submission and get on the leaderboard:\n",
        "\n",
        "1.  Click on the ‚ÄúSubmit Predictions‚Äù button  \n",
        "    ![](https://storage.googleapis.com/kaggle-media/welcome/screen1.png)\n",
        "2.  Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.  \n",
        "    ![](https://storage.googleapis.com/kaggle-media/welcome/screen2.png)\n",
        "\n",
        "### Submission File Format:\n",
        "\n",
        "You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n",
        "\n",
        "The file should have exactly 2 columns:\n",
        "\n",
        "*   `PassengerId` (sorted in any order)\n",
        "*   `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n",
        "\n",
        "###  Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n",
        "\n",
        "*   For Competition Help: [Titanic Discussion Forum](https://www.kaggle.com/c/titanic/discussion)\n",
        "*   Technical Help: [Kaggle Contact Us Page](https://www.kaggle.com/contact)\n",
        "\n",
        "Kaggle doesn‚Äôt have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n",
        "\n",
        "### A Last Word on Kaggle Notebooks\n",
        "\n",
        "\n",
        "As we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data & code.\n",
        "\n",
        "In every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks [here](https://www.kaggle.com/c/titanic/notebooks).\n",
        "\n",
        ">üèÉ‚Äç‚ôÄReady to Compete? [Join the Competition Here!](https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqAhc9lwDprQ"
      },
      "source": [
        "### Goal\n",
        "\n",
        "\n",
        "It is your job to predict if a passenger survived the sinking of the Titanic or not.  \n",
        "For each in the test set, you must predict a 0 or 1 value for the variable.\n",
        "\n",
        "###  Metric\n",
        "\n",
        "Your score is the percentage of passengers you correctly predict. This is known as [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification).\n",
        "\n",
        "### Submission File Format\n",
        "\n",
        "\n",
        "You should submit a csv file with exactly 418 entries **plus** a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.  \n",
        "  \n",
        "The file should have exactly 2 columns:\n",
        "\n",
        "*   PassengerId (sorted in any order)\n",
        "*   Survived (contains your binary predictions: 1 for survived, 0 for deceased)\n",
        "\n",
        "**PassengerId,Survived**  \n",
        "```\n",
        "892,0  \n",
        "893,1  \n",
        "894,0  \n",
        "Etc.\n",
        "```\n",
        "\n",
        "You can download an example submission file (gender\\_submission.csv) on the [Data page](https://www.kaggle.com/c/titanic/data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVv6QTVeDprQ"
      },
      "source": [
        "### Hints\n",
        "\n",
        "* Esto corresponde a un desafio de Kaggle ([link](https://www.kaggle.com/competitions/titanic/overview/description)).\n",
        "* La informaci√≥n respecto a los datos, lo pueden encontrar en el siguiente [link](https://www.kaggle.com/competitions/titanic/data).\n",
        "* A modo de inspiraci√≥n, pueden ocupar algunos gr√°ficos de otros participantes del desaf√≠o ([link](https://www.kaggle.com/competitions/titanic/code))."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librerias y carga de datos"
      ],
      "metadata": {
        "id": "jfEetweLRqlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "80MMyBJuDprQ"
      },
      "outputs": [],
      "source": [
        "## base imports ##\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## cargando los datos ##\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/DatosTarea2Parte2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-5aOVRYHW-e",
        "outputId": "a4c19ce7-b3e7-4141-d167-18f8d7b56de6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gender_submission_df = pd.read_csv(path+\"gender_submission.csv\") #este es un ejemplo de submission\n",
        "test_df = pd.read_csv(path+\"test.csv\") #datos a testear\n",
        "train_df = pd.read_csv(path+\"train.csv\") #datos para entrenar"
      ],
      "metadata": {
        "id": "jRdaduIPHms5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tratamiento de los datos"
      ],
      "metadata": {
        "id": "6wsJiOzy8CkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬øTenemos un problema desbalanceado?\n",
        "R: Si, el ratio de supervivientes/fallecidos es aproximadamente 1.6"
      ],
      "metadata": {
        "id": "2qSRk6p98SX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "549/342"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgT3w43WSIUl",
        "outputId": "d7793aff-ed6c-4ae7-8d08-ad350199b65f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.605263157894737"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Survived'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYC7WG5jtRZs",
        "outputId": "451cfe9d-7f28-4d0b-b0a3-0a891d7b5687"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    549\n",
              "1    342\n",
              "Name: Survived, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contemos los NaNs de train_df y test_df."
      ],
      "metadata": {
        "id": "yH5l1kO48oFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('train_df:')\n",
        "print(train_df.isna().sum())\n",
        "print('\\n')\n",
        "print('test_df:')\n",
        "print(test_df.isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZPbbvMSIuwD",
        "outputId": "a55259af-29bc-4ffe-ff7f-7e972baebfe6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df:\n",
            "PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "test_df:\n",
            "PassengerId      0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age             86\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             1\n",
            "Cabin          327\n",
            "Embarked         0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notamos que la mayoria de NaNs estan en la edad y cabina del train y test, mientras que los otros est√°n en Embarked del train y Fare del test"
      ],
      "metadata": {
        "id": "bZf7TvuzSR0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para los NaNs de la cabina, agregaremos una caracteristica que indica si posee o no cabina, al final, dropearemos la columna Cabin."
      ],
      "metadata": {
        "id": "gMYv9ANMTTvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Has_Cabin'] = train_df[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
        "test_df['Has_Cabin'] = test_df[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)"
      ],
      "metadata": {
        "id": "12xvzxvHTazq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reemplazamos los NaNs de la edad por la media"
      ],
      "metadata": {
        "id": "a_E_i2BzUtDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = [train_df, test_df]\n",
        "for dataset in full_data:\n",
        "    age_avg = dataset['Age'].mean()\n",
        "    dataset['Age'][np.isnan(dataset['Age'])] = age_avg\n",
        "    dataset['Age'] = dataset['Age'].astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9JPKiQNUxba",
        "outputId": "c08c89be-e4f8-4449-9d9e-d85dce1d7d8b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-409d020f6130>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset['Age'][np.isnan(dataset['Age'])] = age_avg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reemplazamos los NaNs de Fare por la media"
      ],
      "metadata": {
        "id": "zuOv5CWPV9dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in full_data:\n",
        "    dataset['Fare'] = dataset['Fare'].fillna(train_df['Fare'].mean())"
      ],
      "metadata": {
        "id": "1FeKlN1QWBbN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rellenamos los NaNs de Embarked"
      ],
      "metadata": {
        "id": "T6Y6VdzpVoa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in full_data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].fillna('S')"
      ],
      "metadata": {
        "id": "Bzsp-6eyVq2t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Botemos algunas columnas"
      ],
      "metadata": {
        "id": "NtzKl4oKWcZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_elements = ['Cabin']\n",
        "train_df = train_df.drop(drop_elements,axis=1)\n",
        "test_df = test_df.drop(drop_elements,axis=1)"
      ],
      "metadata": {
        "id": "mqBUjwxCWuJw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, ya no hay NaNs"
      ],
      "metadata": {
        "id": "m6GN6sB2W6Oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('train_df:')\n",
        "print(train_df.isna().sum())\n",
        "print('\\n')\n",
        "print('test_df:')\n",
        "print(test_df.isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKzuFh54W5GC",
        "outputId": "437766a1-1db6-4324-c03d-275c32f721ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df:\n",
            "PassengerId    0\n",
            "Survived       0\n",
            "Pclass         0\n",
            "Name           0\n",
            "Sex            0\n",
            "Age            0\n",
            "SibSp          0\n",
            "Parch          0\n",
            "Ticket         0\n",
            "Fare           0\n",
            "Embarked       0\n",
            "Has_Cabin      0\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "test_df:\n",
            "PassengerId    0\n",
            "Pclass         0\n",
            "Name           0\n",
            "Sex            0\n",
            "Age            0\n",
            "SibSp          0\n",
            "Parch          0\n",
            "Ticket         0\n",
            "Fare           0\n",
            "Embarked       0\n",
            "Has_Cabin      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procedemos a procesar los datos de tipo categorico.\n",
        "\n",
        "Primero, codificamos los Tickets"
      ],
      "metadata": {
        "id": "HbKEbQeBW_hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Ticket_type'] = train_df['Ticket'].apply(lambda x: x[0:3])\n",
        "train_df['Ticket_type'] = train_df['Ticket_type'].astype('category')\n",
        "train_df['Ticket_type'] = train_df['Ticket_type'].cat.codes\n",
        "\n",
        "test_df['Ticket_type'] = test_df['Ticket'].apply(lambda x: x[0:3])\n",
        "test_df['Ticket_type'] = test_df['Ticket_type'].astype('category')\n",
        "test_df['Ticket_type'] = test_df['Ticket_type'].cat.codes\n",
        "\n",
        "train_df = train_df.drop('Ticket',axis=1)\n",
        "test_df = test_df.drop('Ticket',axis=1)"
      ],
      "metadata": {
        "id": "Ww9cUntTXQs4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del nombre, agregamos el numero de palabras"
      ],
      "metadata": {
        "id": "RRPGluuKXr42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Words_Count'] = train_df['Name'].apply(lambda x: len(x.split()))\n",
        "test_df['Words_Count'] = test_df['Name'].apply(lambda x: len(x.split()))"
      ],
      "metadata": {
        "id": "YJnQO4L_XvQO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambien, agregamos el Titulo con el que empiezan los nombres"
      ],
      "metadata": {
        "id": "S7DR5zOZYeKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_title(name):\n",
        "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
        "    # Si el titulo existe, extraer\n",
        "    if title_search:\n",
        "        return title_search.group(1)\n",
        "    return \"\"\n",
        "# Agregar la nueva col\n",
        "train_df['Title'] = train_df['Name'].apply(get_title)\n",
        "test_df['Title'] = test_df['Name'].apply(get_title)\n",
        "# Agrupar los titulos raros como \"Rare\"\n",
        "\n",
        "train_df['Title'] = train_df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "train_df['Title'] = train_df['Title'].replace('Mlle', 'Miss')\n",
        "train_df['Title'] = train_df['Title'].replace('Ms', 'Miss')\n",
        "train_df['Title'] = train_df['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "test_df['Title'] = test_df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "test_df['Title'] = test_df['Title'].replace('Mlle', 'Miss')\n",
        "test_df['Title'] = test_df['Title'].replace('Ms', 'Miss')\n",
        "test_df['Title'] = test_df['Title'].replace('Mme', 'Mrs')"
      ],
      "metadata": {
        "id": "qZHUD-LMYii1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "de 'SibSp' y 'Parch' agregamos el atributo del tama√±o familiar"
      ],
      "metadata": {
        "id": "U07ZwAWmY42O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\n",
        "test_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1"
      ],
      "metadata": {
        "id": "IBH_7GdvY4T8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, encodeamos los datos categoricos que quedaron"
      ],
      "metadata": {
        "id": "4JJsFUJEZCCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Sex'] = train_df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
        "test_df['Sex'] = test_df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
        "\n",
        "\n",
        "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "train_df['Title'] = train_df['Title'].map(title_mapping)\n",
        "train_df['Title'] = train_df['Title'].fillna(0)\n",
        "test_df['Title'] = test_df['Title'].map(title_mapping)\n",
        "test_df['Title'] = test_df['Title'].fillna(0)\n",
        "\n",
        "train_df['Embarked'] = train_df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
        "test_df['Embarked'] = test_df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
        "\n"
      ],
      "metadata": {
        "id": "w6iYLbPTZBl4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "y eliminamos los atributos que no usaremos"
      ],
      "metadata": {
        "id": "HLsd1LscZg9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_elements = [ 'Name', 'SibSp']\n",
        "train_df = train_df.drop(drop_elements, axis = 1)\n",
        "test_df  = test_df.drop(drop_elements, axis = 1)"
      ],
      "metadata": {
        "id": "zDtaifJaZkT9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "mpz0RueK8LJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC(kernel='rbf',C=1000)\n",
        "grad = GradientBoostingClassifier(n_estimators=10000,learning_rate=0.0005,max_depth=3)\n",
        "X_train,X_test,y_train,y_test = train_test_split(train_df.drop(['PassengerId','Survived'],axis=1),train_df['Survived'],test_size=0.1)\n",
        "svm.fit(X_train,y_train)\n",
        "print('train score:',svm.score(X_train,y_train))\n",
        "print('test score:',svm.score(X_test,y_test))\n",
        "grad.fit(X_train,y_train)\n",
        "print('train score:',grad.score(X_train,y_train))\n",
        "print('test score:',grad.score(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQIxsNgZPRTn",
        "outputId": "e495445c-d37b-4bbb-827d-a0f6c302ad82"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train score: 0.8439450686641697\n",
            "test score: 0.8222222222222222\n",
            "train score: 0.8826466916354557\n",
            "test score: 0.8666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos GradientBoosting, ya que tuvo mejores resultados"
      ],
      "metadata": {
        "id": "J7T8ekKwcR8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission"
      ],
      "metadata": {
        "id": "KGOBTRafcPoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui se prepara la submision"
      ],
      "metadata": {
        "id": "vHMn074ZcXBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columnas = train_df.drop(['PassengerId','Survived'],axis=1).columns.to_list()\n",
        "x_test = test_df[columnas]\n",
        "y_pred = svm.predict(x_test)\n",
        "ids = test_df['PassengerId']\n",
        "submission = pd.DataFrame({'PassengerId':ids,'Survived':y_pred})\n",
        "submission.to_csv('submission.csv',index=False)"
      ],
      "metadata": {
        "id": "-OBHWM6ucZQI"
      },
      "execution_count": 41,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}